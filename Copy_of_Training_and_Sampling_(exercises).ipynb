{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlFTZxgR2u33"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/TransformerLens-intro/main/images/page_images/sampling.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBZfW6rr1_UP"
      },
      "source": [
        "If you have any feedback on this course (e.g. bugs, confusing explanations, parts that you feel could be structured better), please let me know using [this Google Form](https://forms.gle/2ZhdHa87wWsrATjh9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pHA1sTTm4Rt"
      },
      "source": [
        "# Training and Sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkeqdUc883XA"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In the previous set of exercises, we built a transformer from scratch. Here, we're going to look closer at how a transformer works in practice. We'll cover three topics: how to train transformers, how to sample from their output to autoregressively generate text, and how to use caching to run them more efficiently.\n",
        "\n",
        "These exercises mainly focus on building up your understanding of transformers, and the important considerations that go into using them. Subsequent exercises will focus more on interpretability, so you can skip to them if you want (this material generally won't be very important for future exercises)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugWVaR9Gm4Rw"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "Here are the learning objectives for each section of the tutorial. At the end of each section, you should refer back here to check that you've understood everything.\n",
        "\n",
        "## 1️⃣ Training\n",
        "\n",
        "* Review the interpretation of a transformer's output, and learn how it's trained by minimizing cross-entropy loss between predicted and actual next tokens\n",
        "* Construct datasets and dataloaders for the corpus of Shakespeare text\n",
        "* Implement a transformer training loop\n",
        "\n",
        "## 2️⃣ Sampling\n",
        "\n",
        "* Learn how to sample from a transformer\n",
        "    * This includes basic methods like greedy search or top-k, and more advanced methods like beam search\n",
        "\n",
        "## 3️⃣ Caching\n",
        "\n",
        "* Learn how to cache the output of a transformer, so that it can be used to generate text more efficiently\n",
        "* Update your sampling functions to make use of your caching methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mItbKVVm4Rx"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This code includes all the installs and imports you'll need. It also includes code for things like `DemoTransformer`, which you should have done in previous exercises. You are encouraged to copy over your solutions to these exercises, in place of the solutions here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2R_f2joPdh_r"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P_ngQtCbm4Rx",
        "outputId": "4ef05738-7a85-4255-e9b7-cf5ecbb71b80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-1.2.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer_lens)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m327.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.19-py3-none-any.whl (24 kB)\n",
            "Collecting numpy>=1.23 (from transformer_lens)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.3.4)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.65.0)\n",
            "Collecting transformers>=4.25.1 (from transformer_lens)\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.27.1)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.4.0)\n",
            "Collecting aiohttp (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (23.1)\n",
            "Collecting responses<0.19 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (6.0)\n",
            "Collecting typeguard>=2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->transformer_lens)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=cdaa01dd63b304cc7e73e56e3010c4825aa24670c9df5a98caed75f0b8de863f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, xxhash, typeguard, smmap, setproctitle, sentry-sdk, numpy, multidict, frozenlist, fancy-einsum, einops, docker-pycreds, dill, async-timeout, yarl, responses, multiprocess, jaxtyping, huggingface-hub, gitdb, aiosignal, transformers, GitPython, aiohttp, wandb, datasets, transformer_lens\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.1 fancy-einsum-0.0.3 frozenlist-1.3.3 gitdb-4.0.10 huggingface-hub-0.14.1 jaxtyping-0.2.19 multidict-6.0.4 multiprocess-0.70.14 numpy-1.24.3 pathtools-0.1.2 responses-0.18.0 sentry-sdk-1.22.2 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformer_lens-1.2.2 transformers-4.29.1 typeguard-4.0.0 wandb-0.15.2 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.0.0+cu118)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Installing collected packages: torchtyping\n",
            "Successfully installed torchtyping-0.1.4\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  print(\"Running as a Colab notebook\")\n",
        "  %pip install transformer_lens\n",
        "  %pip install torchtyping\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print(\"Running as a Jupyter notebook - intended for development only!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tfWhhdEBm4Ry",
        "outputId": "d01cb777-aeb0-4162-fe52-8f532416ebbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import re\n",
        "from typing import Optional\n",
        "import torch as t\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import transformers\n",
        "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
        "import numpy as np\n",
        "import einops\n",
        "from dataclasses import dataclass\n",
        "from frozendict import frozendict\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "from torchtyping import TensorType as TT\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.utils import gelu_new\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GBhdl0gyHaPb",
        "outputId": "10a06155-78df-4e1a-fea7-73a7cb18b10f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def test_tensor_dataset(TensorDataset):\n",
        "    tensors = [t.rand((10, 20)), t.rand((10, 5)), t.arange(10)]\n",
        "    dataset = TensorDataset(*tensors)\n",
        "    assert len(dataset) == 10\n",
        "    for index in [0, slice(0, 5, 1), slice(1, 5, 2)]:\n",
        "        print(\"Testing with index:\", index)\n",
        "        expected = tuple(tensor[index] for tensor in tensors)\n",
        "        actual = dataset[index]\n",
        "        for e, a in zip(expected, actual):\n",
        "            t.testing.assert_close(e, a)\n",
        "    print(\"All tests in `test_tensor_dataset` passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Uw3MKlNzMZbO",
        "outputId": "19504fa5-9ef5-4f0a-cc8b-ac4a61c7927e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_two_lines(x1=None, y1=None, x2=None, y2=None, name1=\"\", name2=\"\", title=\"\", xaxis=\"\", yaxis=\"\"):\n",
        "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "    if x1 is None: x1 = list(range(len(y1)))\n",
        "    if x2 is None: x2 = list(range(len(y2)))\n",
        "    fig.add_trace(go.Scatter(x=x1, y=y1, name=name1), secondary_y=False)\n",
        "    fig.add_trace(go.Scatter(x=x2, y=y2, name=name2), secondary_y=True)\n",
        "    fig.update_layout(title=title, xaxis_title=xaxis, yaxis_title=yaxis)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EDNJEJx6Wd6V",
        "outputId": "b7918404-d8ef-431e-e1e4-1de389b0e594"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 768\n",
        "    debug: bool = True\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024\n",
        "    d_head: int = 64\n",
        "    d_mlp: int = 3072\n",
        "    n_heads: int = 12\n",
        "    n_layers: int = 12\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
        "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
        "\n",
        "    def forward(self, residual):\n",
        "        # residual: [batch, position, d_model]\n",
        "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
        "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
        "\n",
        "        residual = (residual - residual_mean) / residual_std\n",
        "        return residual * self.w + self.b\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, position]\n",
        "        return self.W_E[tokens]\n",
        "\n",
        "\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, position]\n",
        "        batch, seq_len = tokens.shape\n",
        "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=\"cuda\"))\n",
        "\n",
        "    def forward(self, normalized_resid_pre: t.Tensor):\n",
        "        # normalized_resid_pre: [batch, position, d_model]\n",
        "\n",
        "        # Calculate query, key and value vectors\n",
        "        q = einops.einsum(\n",
        "            normalized_resid_pre, self.W_Q,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\" \n",
        "        ) + self.b_Q\n",
        "        k = einops.einsum(\n",
        "            normalized_resid_pre, self.W_K,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\" \n",
        "        ) + self.b_K\n",
        "        v = einops.einsum(\n",
        "            normalized_resid_pre, self.W_V,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\" \n",
        "        ) + self.b_V\n",
        "\n",
        "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
        "        attn_scores = einops.einsum(\n",
        "            q, k, \n",
        "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\"\n",
        "        )\n",
        "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
        "        attn_pattern = attn_scores_masked.softmax(-1)\n",
        "\n",
        "        # Take weighted sum of value vectors, according to attention probabilities\n",
        "        z = einops.einsum(\n",
        "            v, attn_pattern, \n",
        "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\"\n",
        "        )\n",
        "\n",
        "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "        out = einops.einsum(\n",
        "            z, self.W_O, \n",
        "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\"\n",
        "        ) + self.b_O\n",
        "\n",
        "        return out\n",
        "\n",
        "    def apply_causal_mask(self, attn_scores: t.Tensor):\n",
        "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
        "        seq_len = attn_scores.shape[-1]\n",
        "        q_posn = einops.repeat(attn_scores.new_tensor(range(seq_len)), \"q -> q k\", k=seq_len)\n",
        "        k_posn = einops.repeat(attn_scores.new_tensor(range(seq_len)), \"k -> q k\", q=seq_len)\n",
        "        attn_scores = attn_scores.masked_fill(q_posn < k_posn, self.IGNORE)\n",
        "        return attn_scores\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
        "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "\n",
        "    def forward(self, normalized_resid_mid):\n",
        "        # normalized_resid_mid: [batch, position, d_model]\n",
        "        normalized_resid_mid = normalized_resid_mid @ self.W_in + self.b_in\n",
        "        normalized_resid_mid = gelu_new(normalized_resid_mid)\n",
        "        normalized_resid_mid = normalized_resid_mid @ self.W_out + self.b_out\n",
        "        return normalized_resid_mid\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, resid_pre):\n",
        "        # resid_pre: [batch, position, d_model]\n",
        "        # output: [batch, position, d_model]\n",
        "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
        "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
        "        return resid_post\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "    def forward(self, normalized_resid_final):\n",
        "        # normalized_resid_final [batch, position, d_model]\n",
        "        return einops.einsum(\n",
        "            normalized_resid_final, self.W_U,\n",
        "            \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
        "        ) + self.b_U\n",
        "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
        "\n",
        "\n",
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens [batch, position]\n",
        "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "        logits = self.unembed(self.ln_final(residual))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1x_mWI6m4Rz"
      },
      "source": [
        "# 1️⃣ Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FUMfnwA6Oq"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "* Review the interpretation of a transformer's output, and learn how it's trained by minimizing cross-entropy loss between predicted and actual next tokens\n",
        "* Construct datasets and dataloaders for the corpus of Shakespeare text\n",
        "* Implement a transformer training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d--zV4Crm4Rz"
      },
      "source": [
        "Hopefully, you've now successfully implemented a transformer, and seen how to use it to generate output autoregressively. You might also have seen the example training loop at the end of the last section. Here, you'll train your transformer in a more hands-on way, using the [complete works of William Shakespeare](https://www.gutenberg.org/files/100/100-0.txt).\n",
        "\n",
        "This is the task recommended by Jacob Hilton in his [curriculum](https://github.com/jacobhilton/deep_learning_curriculum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6B61--0GXQ_"
      },
      "source": [
        "## Cross entropy loss\n",
        "\n",
        "Your transformer's input has shape `(batch, seq_len)`, where the `[i, j]`-th element is the token id of the `j`-th token in the `i`-th sequence. Your transformer's output has shape `(batch, seq_len, vocab_size)`, where the `[i, j, :]`-th element is a vector of logits, representing a probability distribution over the token that **follows** the `j`-th token in the `i`-th sequence.\n",
        "\n",
        "When training our model, we use cross-entropy loss between the model's predictions and the actual next tokens. In other words, we can take the `[:, :-1, :]`-th slice of our output (which is a tensor of probability distributions for the **last** `seq_len - 1` tokens in each sequence), and compare this to the `[:, 1:, :]`-th slice (which represents the actual tokens we're trying to predict).\n",
        "\n",
        "In the last section, we saw the function `lm_cross_entropy_loss` which calculated this for us. Let's take another look at this function, so we understand how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mihCSRTTGXCW",
        "outputId": "c3e7c991-0534-4781-e862-8235fa0d1f00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def lm_cross_entropy_loss(logits: t.Tensor, tokens: t.Tensor):\n",
        "    # Measure next token loss\n",
        "    # Logits have shape [batch, position, d_vocab]\n",
        "    # Tokens have shape [batch, position]\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "    return -pred_log_probs.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw0IQFkuGYko"
      },
      "source": [
        "First, we get `log_probs`, which are the log probabilities of each token in the vocab. Log probs are (as you'd probably guess!) the log of the probabilities implied by the logit distribution. We get them from logits by taking softmax, then taking log again (so they're equal to logits, up to a constant difference). If you examine the formula for [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), you'll notice that it's just the negative of the log probability of the correct token.\n",
        "\n",
        "In the second line, we use the `gather` method to take the log probabilities corresponding to the correct token. This is a bit confusing, and you don't need to understand the exact syntax of `gather`. This line of code does the following:\n",
        "* Indexes `log_probs`, taking the `[:, :-1]`-th slice (so we have the logits corresponding to the **last** `seq_len - 1` tokens in each sequence)\n",
        "* Indexes `tokens`, taking the `[:, 1:]`-th slice (so we have the actual tokens we're trying to predict)\n",
        "* Indexes into the reduced `log_probs` tensor using `gather`, so we get the log probabilities of the correct tokens\n",
        "\n",
        "Finally, we take the mean of the negative log probabilities, and return this as our loss. Remember that log probs are always negative (because log of a number less than 1 is negative), so our loss will always be non-negative. It will tend to zero only if our model tends towards assigning 100% probability to the correct token, and 0% to all others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3yCflLrGbGf"
      },
      "source": [
        "## Tokenizers\n",
        "\n",
        "Now that we've got cross entropy loss out of the way, let's start working with our dataset. We'll be using the Shakespeare corpus for this exercises; you can get the text by downloading it from [this link](https://drive.google.com/file/d/1qf4TqyMX9TK6W9MEt_AgfbSLVrENBtYl/view?usp=share_link), then running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qLdfi6-Ra-Em",
        "outputId": "2da37977-d2af-4618-a206-039b79812422"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9df32e5-d69d-442c-aec3-7ce0ef0dc0be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9df32e5-d69d-442c-aec3-7ce0ef0dc0be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 100-0.txt to 100-0.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1suv7HfUGqaK",
        "outputId": "f56d35e5-f287-43e0-893e-c739c2b631ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"100-0.txt\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pniZkS2-GZ3f"
      },
      "source": [
        "You should print out the first few lines of this text, and get a feel for what it looks like.\n",
        "\n",
        "Rather than using a fancy tokenizer, we'll just split the text into tokens using a regular expression. This is a bit crude, but it's good enough for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2pXmWilGvg_"
      },
      "source": [
        "#### Exercise - implement `SimpleTokenizer`\n",
        "\n",
        "Below, you should fill in the `SimpleTokenizer` class. Some guidance for this exercise:\n",
        "\n",
        "#### __init__\n",
        "\n",
        "The `text` argument is meant to be a string (this will be the same as the `text` object you defined above). Here, you should define `self.words` as a list of all the different tokens that appear in the text, sorted in some reasonable way (you can split the text with `re.split(r\"\\b\", text))`). You should then define `self.word_to_index` and `self.index_to_word`, which are dictionaries that map tokens to their token ids, and vice-versa (with the token ids being the positions of the tokens in `self.words`).\n",
        "\n",
        "Also, it's good practice to include an unknown token `unk` in your vocabulary, just in case you feed the model a token that it hasn't seen before (you can give it the index one larger than the largest in your words list). We won't bother using a start token here (although you might want to think about doing this, as a bonus exercise).\n",
        "\n",
        "#### `encode`\n",
        "\n",
        "This takes in some text, and returns tokens. If `return_tensors` is None (the default), this should return a simple list of integers. If `return_tensors == \"pt\"`, this should return a PyTorch tensor of shape `(1, seq_len)` (it's good practice to always add a batch dimension, even if there's only one sequence in the batch).\n",
        "\n",
        "If the input text contains an unknown token, then you can print an error message (or raise an exception).\n",
        "\n",
        "#### `decode`\n",
        "\n",
        "Finally, this should take in a list or tensor of tokens (you can assume that the batch dimension will be 1 if it's a tensor), and returns a string of the decoded text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "APPzb4_IGx9S",
        "outputId": "c694006d-bc6b-40d3-fd77-7fcc8bd9d97f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class SimpleTokenizer():\n",
        "\n",
        "    def __init__(self, text: str):\n",
        "        self.words = list(set(re.split(r'\\b', text)))\n",
        "        self.word_to_index = {w : self.words.index(w) for w in self.words}\n",
        "        self.word_to_index['unk'] = len(self.words)\n",
        "        self.word_to_index['<start>'] = len(self.words) + 1\n",
        "        self.index_to_word = {value : key for key, value in self.word_to_index.items()}\n",
        "\n",
        "    def encode(self, input_text, return_tensors: Optional[str] = None) -> Union[List, t.Tensor]:\n",
        "        '''\n",
        "        Tokenizes and encodes the input text.\n",
        "\n",
        "        If `return_tensors` is None, should return list of Python integers.\n",
        "        If `return_tensors` is \"pt\", should return a PyTorch tensor of shape (1, num_tokens).\n",
        "        '''\n",
        "\n",
        "        token_list = input_text.split(' ')\n",
        "        tokenized_list = [self.word_to_index.get(tok, self.word_to_index['unk']) for tok in token_list]\n",
        "        if return_tensors is None:\n",
        "          return tokenized_list\n",
        "        else:\n",
        "          return t.tensor(tokenized_list).unsqueeze(0)\n",
        "\n",
        "\n",
        "    def decode(self, tokens: Union[List, t.Tensor]):\n",
        "        '''\n",
        "        Decodes the tokens into a string of text.\n",
        "        '''\n",
        "        if isinstance(tokens, t.Tensor) and tokens.dim() == 2:\n",
        "          assert tokens.size(0) == 1, \"Only batch size 1 is supported\"\n",
        "          tokens = tokens[0]\n",
        "          tokens = tokens.tolist()\n",
        "        \n",
        "        str_ = ' '.join([self.index_to_word[tok] for tok in tokens])\n",
        "        return str_\n",
        "\n",
        "\n",
        "mytokenizer = SimpleTokenizer(text)\n",
        "\n",
        "# Some basic testing\n",
        "assert isinstance(mytokenizer.encode(\"Macbeth\"), list)\n",
        "assert isinstance(mytokenizer.encode(\"Macbeth\", return_tensors=\"pt\"), t.Tensor)\n",
        "assert mytokenizer.decode(mytokenizer.encode(\"Macbeth\")) == \"Macbeth\"\n",
        "assert mytokenizer.index_to_word[mytokenizer.encode(\"Macbeth\")[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSsizAK5G-Y2"
      },
      "source": [
        "## Preparing text\n",
        "\n",
        "We have our tokenizer, but we still need to be able to take in our `text` object and turn it into a tensor of token ids, without any of them overlapping. This is important because overlapping sequences might cause use to double-count certain sequences during training, and will make it seem like our model is learning faster than it really is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dHSRJYkHEDn"
      },
      "source": [
        "#### Exercise - implement `prepare_text`\n",
        "\n",
        "Below, you should fill in the `prepare_text` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "d6zYQWIbHBr7",
        "outputId": "f0f3c8d3-afd8-4c04-e0c4-4e72af589af9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does this size look reasonable, as a tokenization of the first 500 characters?\n",
            " torch.Size([1, 48])\n"
          ]
        }
      ],
      "source": [
        "def prepare_text(text: str, max_seq_len: int, tokenizer: SimpleTokenizer):\n",
        "    '''\n",
        "    Takes a string of text, and returns an array of tokens rearranged into chunks of size max_seq_len.\n",
        "    '''\n",
        "\n",
        "    tokenized_text = tokenizer.encode(text, return_tensors = 'pt')\n",
        "    batch_size = tokenized_text.shape[1] // max_seq_len\n",
        "    tokenized_text = tokenized_text[0, : batch_size * max_seq_len]\n",
        "    return tokenized_text.view(batch_size, max_seq_len)\n",
        "\n",
        "max_seq_len=48\n",
        "tokens = prepare_text(text[:500], max_seq_len=max_seq_len, tokenizer=mytokenizer)\n",
        "print(\"Does this size look reasonable, as a tokenization of the first 500 characters?\\n\", tokens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK4gIHGyHMxs"
      },
      "source": [
        "## Datasets and Dataloaders\n",
        "\n",
        "### Build Your Own TensorDataset\n",
        "\n",
        "The class `torch.utils.data.dataset.TensorDataset` is a convenient wrapper for passing around multiple tensors that have the same size in the first dimension. The most common example of this is in supervised learning, where you have one tensor of inputs and a second tensor with corresponding labels. Often these tensors will have different `dtype`s, so it doesn't make sense to `torch.stack` them into one big tensor, and it be cumbersome to pass them around as separate variables or as a tuple.\n",
        "\n",
        "`TensorDataset` accepts and stores any number of tensors in the constructor along with implementing `__getitem__` so that `my_dataset[n]` returns a tuple containing element `n` from each stored `Tensor`. Similarly, `my_dataset[:5]` returns a tuple containing the first five elements from each stored `Tensor`.\n",
        "\n",
        "### Slice Objects in Python\n",
        "\n",
        "`slice` is a built-in type containing `start`, `stop`, and `step` fields which can be integers or `None`. Given `x=[1,2,3,4,5,6,7]`, writing `x[1:5:2]` is syntactic sugar for `x[slice(1, 5, 2)]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWkgx_veHQVt"
      },
      "source": [
        "#### Exercise - implement `TensorDataset`\n",
        "\n",
        "*This should be a relatively unchallenging exercise, and you can skip it if it doesn't seem interesting to you.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDI_3ug7HTGf"
      },
      "source": [
        "You should fill in the methods below, and verify that the tests pass.\n",
        "\n",
        "Note that we're only passing in one tensor to this class (the `tokens` tensor), but this class should also be able to accept multiple tensors (this will be useful when we get to some later examples, like training models to solve algorithmic tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "Zule-aQmHVM4",
        "outputId": "4ec8c2a8-9bc7-4343-cf10-11ebcef50a5a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with index: 0\n",
            "Testing with index: slice(0, 5, 1)\n",
            "Testing with index: slice(1, 5, 2)\n",
            "All tests in `test_tensor_dataset` passed!\n"
          ]
        }
      ],
      "source": [
        "class TensorDataset:\n",
        "    def __init__(self, *tensors: t.Tensor):\n",
        "        '''Validate the sizes and store the tensors in a field named `tensors`.'''\n",
        "        batch_sizes = [tensor.shape[0] for tensor in tensors]\n",
        "        assert len(set(batch_sizes)) == 1, \"All tensors must have the same size in the first dimension\"\n",
        "\n",
        "        self.data = tensors\n",
        "\n",
        "    def __getitem__(self, index: Union[int, slice]) -> Tuple[t.Tensor, ...]:\n",
        "        '''Return a tuple of length len(self.tensors) with the index applied to each.'''\n",
        "        \n",
        "        return tuple(tensor[index] for tensor in self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Return the size in the first dimension, common to all the tensors.'''\n",
        "        \n",
        "        return self.data[0].shape[0]\n",
        "\n",
        "\n",
        "test_tensor_dataset(TensorDataset)\n",
        "\n",
        "dataset = TensorDataset(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvFy4Xf4HhjD"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Now, it's time for our training loop! We've left this exercise very open-ended, like our implementation of the ResNet training loop in last week's exercises. The principles are exactly the same, and we've provided you with a skeleton of the function to help get you started. \n",
        "\n",
        "Again, we use a `dataclass` object to store the training parameters, because this is a useful way of keeping your code organised. Note one extra feature here - rather than defining our `optimizer_kwargs` object as a dictionary, we define it as a `frozendict` (which is a special dataclass that works just like regular dicts, except that it isn't mutable). This is a helpful way to get around the fact that you aren't allowed to set dataclass fields to mutable object like dictionaries or lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztruwcy_Hj7N"
      },
      "source": [
        "#### Exercise - write a training loop\n",
        "\n",
        "You should read and understand the code below, and fill in the section marked `YOUR CODE HERE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FkwI1OSFHmGi",
        "outputId": "3871ef52-8ecb-4da5-af2e-de53dde56dd0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "@dataclass\n",
        "class TransformerTrainingArgs():\n",
        "    tokenizer: SimpleTokenizer = mytokenizer\n",
        "    epochs: int = 3\n",
        "    batch_size: int = 4\n",
        "    max_seq_len: int = 48\n",
        "    optimizer: Callable[..., t.optim.Optimizer] = t.optim.Adam\n",
        "    optimizer_kwargs: Dict = frozendict(lr=0.001, betas=(0.9, 0.999))\n",
        "    device: str = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
        "    filename_save_model: str = \"transformer_shakespeare.pt\"\n",
        "\n",
        "    \n",
        "def train_transformer(model: DemoTransformer, text: str, args: TransformerTrainingArgs) -> Tuple[list, list]:\n",
        "    '''\n",
        "    Trains an autoregressive transformer on the data in the trainset.\n",
        "\n",
        "    Returns tuple of (train_loss, test_loss), containing the cross entropy losses for the thing.\n",
        "    '''\n",
        "    # Prepare the tokens, take a random train/test split, and create the dataloaders\n",
        "    tokens = prepare_text(text, max_seq_len=args.max_seq_len, tokenizer=args.tokenizer)\n",
        "    randperm = t.randperm(tokens.size(0))\n",
        "    len_trainset = int(0.9 * tokens.size(0))\n",
        "    trainset = TensorDataset(tokens[randperm[:len_trainset]])\n",
        "    testset = TensorDataset(tokens[randperm[len_trainset:]])\n",
        "    trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
        "    testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    model.to(args.device)\n",
        "    optimizer = args.optimizer(model.parameters(), **args.optimizer_kwargs)\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    test_losses = []\n",
        "    # YOUR CODE HERE - implement training and testing loops\n",
        "    best_test_loss = 100000\n",
        "    for epoch in range(args.epochs):\n",
        "\n",
        "      train_losses = []\n",
        "      for tokens in tqdm(trainloader):\n",
        "    \n",
        "        tokens = tokens[0]\n",
        "        tokens = tokens.to(args.device)\n",
        "        logits = model(tokens)\n",
        "        loss = lm_cross_entropy_loss(logits, tokens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "      average_loss = sum(train_losses) / len(train_losses)\n",
        "      train_loss_list.append(average_loss)\n",
        "      print(f'Average train loss after epoch {epoch+1} is {average_loss}')\n",
        "      with t.inference_mode():\n",
        "\n",
        "        test_loss = 0\n",
        "        total = 0\n",
        "        for step, tokens in enumerate(testloader):\n",
        "          tokens = tokens[0]\n",
        "          tokens = tokens.to(args.device)\n",
        "          logits = model(tokens)\n",
        "          loss = lm_cross_entropy_loss(logits, tokens)\n",
        "          test_loss += lm_cross_entropy_loss(logits, tokens) * tokens.size(0)\n",
        "          total += tokens.size(0)\n",
        "\n",
        "        test_loss /= total\n",
        "        print(f'Average test loss after epoch {epoch+1} is {test_loss}')\n",
        "        test_loss_list.append(test_loss.item())  \n",
        "        if test_loss.item() < best_test_loss:\n",
        "          best_test_loss = test_loss\n",
        "          print(f\"\\nSaving model to: {args.filename_save_model}\")\n",
        "          t.save(model, args.filename_save_model)\n",
        "\n",
        "    return train_loss_list, test_loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_ozEtiJHmnU"
      },
      "source": [
        "You can take a look at the solutions for an example implementation (although it's totally fine to have something which looks different to this).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93g1e2NYOT6"
      },
      "source": [
        "Once you've written a training loop, you can run it (and plot your output) with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "YjuTuG4WHq90",
        "outputId": "99c7edcf-77f8-48d9-a872-e8a9cc9cb882"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2202/2202 [01:44<00:00, 21.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss after epoch 1 is 4.646601952172539\n",
            "Average test loss after epoch 1 is 4.529391765594482\n",
            "\n",
            "Saving model to: transformer_shakespeare.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2202/2202 [01:43<00:00, 21.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss after epoch 2 is 4.3964127488400475\n",
            "Average test loss after epoch 2 is 4.465266227722168\n",
            "\n",
            "Saving model to: transformer_shakespeare.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2202/2202 [01:47<00:00, 20.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss after epoch 3 is 4.297598929647745\n",
            "Average test loss after epoch 3 is 4.446091651916504\n",
            "\n",
            "Saving model to: transformer_shakespeare.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"1ed2574a-991a-4f65-b54e-676496c4e47f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1ed2574a-991a-4f65-b54e-676496c4e47f\")) {                    Plotly.newPlot(                        \"1ed2574a-991a-4f65-b54e-676496c4e47f\",                        [{\"name\":\"Train loss\",\"x\":[0,1,2],\"y\":[4.646601952172539,4.3964127488400475,4.297598929647745],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"name\":\"Test loss\",\"x\":[1,2,3],\"y\":[4.529391765594482,4.465266227722168,4.446091651916504],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94],\"title\":{\"text\":\"Batches seen\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cross entropy loss\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Loss curve for transformer trained on Shakespeare corpus\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1ed2574a-991a-4f65-b54e-676496c4e47f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "config = Config(\n",
        "    d_model = 384,\n",
        "    layer_norm_eps = 1e-5,\n",
        "    d_vocab = 50257,\n",
        "    init_range = 0.02,\n",
        "    n_ctx = 1024,\n",
        "    d_head = 64,\n",
        "    d_mlp = 1536,\n",
        "    n_heads = 6,\n",
        "    n_layers = 4\n",
        ")\n",
        "\n",
        "model = DemoTransformer(config)\n",
        "\n",
        "args = TransformerTrainingArgs(\n",
        "    tokenizer = mytokenizer,\n",
        "    batch_size = 8,\n",
        "    epochs = 3,\n",
        ")\n",
        "\n",
        "train_loss_list, test_loss_list = train_transformer(model, text, args)\n",
        "\n",
        "plot_two_lines(\n",
        "    y1 = train_loss_list,\n",
        "    y2 = test_loss_list,\n",
        "    x2 = list(range(\n",
        "        len(train_loss_list) // len(test_loss_list), \n",
        "        len(train_loss_list) + 1,\n",
        "        len(train_loss_list) // len(test_loss_list)\n",
        "    )),\n",
        "    name1 = \"Train loss\",\n",
        "    name2 = \"Test loss\",\n",
        "    title = \"Loss curve for transformer trained on Shakespeare corpus\",\n",
        "    xaxis = \"Batches seen\",\n",
        "    yaxis = \"Cross entropy loss\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKDInVVOHwdR"
      },
      "source": [
        "You can try playing around with some of the hyperparameters, and see how they affect the training process. You might also want to try out using different datasets (there are many online you can use!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZEow3yH3W8"
      },
      "source": [
        "# 2️⃣ Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZL61SooH7J-"
      },
      "source": [
        "#### Learning Objectives\n",
        "\n",
        "* Learn how to sample from a transformer\n",
        "    * This includes basic methods like greedy search or top-k, and more advanced methods like beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt5OsRyHH-Tm"
      },
      "source": [
        "One obvious method to sample tokens from a distribution would be to always take the token assigned the highest probability. But this can lead to some boring and repetitive outcomes, and at worst it can lock our transformer's output into a loop.\n",
        "\n",
        "First, you should read HuggingFace's blog post [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Once you've done that, we've included some exercises below that will allow you to write your own methods for sampling from a transformer. You'll be working with a pretrained model rather than the Shakespeare model in the previous set of exercises (because sampling can behave quite unpredictably unless tokenization and training are done very carefully), although you might want to try substituting in your Shakespeare model to these exercises if you have extra time at the end, and see how it behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "6b805df6f8a84f4897564f152ccb62d6",
            "d3cd3727f92c480885c47d80eb36ff83",
            "612eade057c0451ab59492aa873b6be3",
            "e1b05088b1dc4044b03f9e88311d1e21",
            "1f53a0ddd50c4decaefacffd374854cc",
            "75e6da06147949118c6f60a62765d07f",
            "f3e34100a91b432fb584575524b0ca1d",
            "c3a9a3cbd7bd4c5ba70a6e225528b7b0",
            "c1811a9693f7435c9d3ddf96f518a1d5",
            "0540262086bb47b18395e9d532cc7e46",
            "19e2ff04624f40da8badc19f0d51656f",
            "f9a3ef80dcf84c83a3c365eb3a025655",
            "d652abc12496470a8c59e3577252c6cc",
            "e59f46865a3843da82da5879ed577fec",
            "988392326d424718aa544f726a0d7168",
            "8267ce0a547f44e5bd2362d86f405e55",
            "6077381c0b984bc5b4891d3f68edce0d",
            "d625cfda6f2d48dc96197104f2bcc28a",
            "882af7fbe28a456781d6f3efc8f967e3",
            "4624804f4db94a7b9bd7d1b449be872a",
            "7e4c379b742f4153bd5d5d85b29405df",
            "090a181096464e9e876b4aa0a052f406",
            "26f332178bea48ce9aee1b003b567329",
            "345355ba2b2d4218bd00d6e468cf2965",
            "2f8ae10bbee94b809bdfe40b44e4ca38",
            "efe859945acc43589bc0919383ac595f",
            "477b22e3d2844d66910f0de6fee25064",
            "4ed6d846349641608dd2a70e0bb192cc",
            "0b1f665ecfdd4d3db28ae68acbaf2229",
            "33c8678ed23848d992481298a9a24943",
            "4e2b88fe75a64233a54d688e9132fc21",
            "0d75c860a320414a84a4b4a786c4b0ee",
            "690914c6b7084183b8292519feae2c47",
            "b1a88370757d49c79515f85167640add",
            "df64c2cc13f748599b858f4bcd67f4e4",
            "044d7f7cf69b40bb89c302a31bb78491",
            "4840ef7dab5740ed8f1762451088f2a8",
            "867f8524eb0a475f95ea21032ebbd193",
            "5aaf9599f2c44d35bb7ebf25c9d11ed2",
            "ec1188635d214e13a9245a7e9188050a",
            "83252e75857149ce9d5a211966b2dc96",
            "b9584018a4ae4ea3a8cbf9d4264da4d3",
            "aa8bd7d210574bab94a42f341d436aed",
            "1ed9becf4af547a38b6c704ad63d71f0",
            "4a6379653efc489aa3bdd26773cb2490",
            "6e29bcb0b09e4f7f8faa8378f8670c74",
            "704eac3f771846479d11fef160a71bac",
            "5ec84f20faae40118e6051f322bc1881",
            "d9e282669341488a9ce57c3923f2bc2a",
            "99029029d15a4cf1afc41a2603d08294",
            "42b609e1c91542d89efd98f95deb4e3d",
            "590b6a3606ac4616af104773b27b7d15",
            "b68df2df8f4f425a96fa1b4475e539de",
            "0fbdda9dc7c64362b6047e239d0d2f2d",
            "2433d41c34fc4d4f82cc23179508236b",
            "cb68321b4ca34f3b8d48373d58c0d99a",
            "f75d943de605474b846dcdc50a190baf",
            "df4aacaeda724537ab710685ff9a8148",
            "a5b9066961b749bab3b337c9faf8e8ed",
            "a309cb87c66143c7ba9d33404983d510",
            "c7e951b3b0d4428bb963f94a4514d20f",
            "969abd4a40234309b0c422e1a8315cdd",
            "330352ccf69048bdab6410304f83a9b4",
            "284d7765ba0047b8bbe2a3847a0e3d4b",
            "bb903806a7c04833b94c356062021750",
            "c5de1a5a70ba47af82dc9f98e7334f1f"
          ]
        },
        "id": "WuaD748TH_SI",
        "outputId": "3ad9222b-b78c-4584-8520-69b001fb6c00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b805df6f8a84f4897564f152ccb62d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9a3ef80dcf84c83a3c365eb3a025655"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26f332178bea48ce9aee1b003b567329"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1a88370757d49c79515f85167640add"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a6379653efc489aa3bdd26773cb2490"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb68321b4ca34f3b8d48373d58c0d99a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
        "gpt2 = DemoTransformer(Config())\n",
        "gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
        "gpt2.cuda()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgV7AbH8ICTb"
      },
      "source": [
        "## Sampling Boilerplate\n",
        "\n",
        "The provided functions `apply_sampling_methods` and `sample_tokens` include the boilerplate for sampling from the model. Note that there is a special token `tokenizer.eos_token`, which during training was added to the end of a each article. GPT-2 will generate this token when it feels like the continuation is at a reasonable stopping point, which is our cue to stop generation.\n",
        "\n",
        "The functions called in `apply_sampling_methods` are not defined yet - you are going to implement them below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OmdO0E1dIGty",
        "outputId": "c3236671-04ac-430b-f21f-dff537e3d70f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def apply_sampling_methods(\n",
        "    input_ids: t.Tensor, \n",
        "    logits: t.Tensor, \n",
        "    temperature=1.0, \n",
        "    freq_penalty=0.0, \n",
        "    top_k=0, \n",
        "    top_p=0.0,\n",
        "    seed=0\n",
        ") -> int:\n",
        "    '''\n",
        "    Return the next token, sampled from the model's probability distribution with modifiers.\n",
        "    input_ids: shape (seq,)\n",
        "    '''\n",
        "    assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
        "    assert temperature >= 0, \"Temperature should be non-negative\"\n",
        "    assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
        "    assert 0 <= top_k, \"Top-k must be non-negative\"\n",
        "    assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    t.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    if temperature == 0:\n",
        "        return greedy_search(logits)\n",
        "    if temperature != 1.0:\n",
        "        logits = apply_temperature(logits, temperature)\n",
        "    if freq_penalty != 0.0:\n",
        "        logits = apply_freq_penalty(input_ids, logits, freq_penalty)\n",
        "    if top_k > 0:\n",
        "        return sample_top_k(logits, top_k)\n",
        "    if top_p > 0:\n",
        "        return sample_top_p(logits, top_p)\n",
        "    return sample_basic(logits)\n",
        "\n",
        "\n",
        "@t.inference_mode()\n",
        "def sample_tokens(\n",
        "    model: DemoTransformer,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    initial_text: str,\n",
        "    max_tokens_generated=30,\n",
        "    **kwargs # kwargs are for params like temperature, top_k, etc\n",
        ") -> str:\n",
        "    '''\n",
        "    Sample tokens until the model outputs `tokenizer.eos_token_id` or the specified token limit is reached.\n",
        "\n",
        "    Return: the prompt and continuation concatenated\n",
        "    '''\n",
        "    model.eval()\n",
        "    input_ids: list = tokenizer.encode(initial_text)\n",
        "    generated = []\n",
        "    for _ in range(max_tokens_generated):\n",
        "        new_input_ids = t.tensor(input_ids + generated, dtype=t.long, device=\"cuda\")\n",
        "        new_input_ids_window = new_input_ids[-min(model.cfg.n_ctx, new_input_ids.shape[0]):].unsqueeze(0)\n",
        "        logits = model(new_input_ids_window)[0, -1]\n",
        "        new_token = apply_sampling_methods(new_input_ids, logits, **kwargs)\n",
        "        generated.append(new_token)\n",
        "        if new_token == getattr(tokenizer, \"eos_token_id\", None):\n",
        "            break\n",
        "    return tokenizer.decode(input_ids + generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOHwnv2IGCS"
      },
      "source": [
        "A few notes on this function:\n",
        "\n",
        "* We use `tokenizer.encode` to convert the initial text string into a list of logits. You can also pass the argument `return_tensors=\"pt\"` in order to return the output as a tensor.\n",
        "* `new_input_ids` is a concatenation of the original input ids, and the ones that have been autoregressively generated.\n",
        "* `new_input_ids_truncated` truncates `new_input_ids` at `max_seq_len` (because you might get an error at the positional embedding stage if your input sequence length is too large).\n",
        "* The line `all_logits = ...` is necessary because HuggingFace's GPT doesn't just output logits, it outputs an object which contains `logits` and `past_key_values`. In contrast, your model will probably just output logits, so we can directly define logits as the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgFtg-4QIRrl"
      },
      "source": [
        "<details>\n",
        "<summary>Question - why do you think we take <code>logits[0, -1]</code> ?</summary>\n",
        "\n",
        "Our model input has shape `(batch, seq_len)`, and each element is a token id. Our output has dimension `(batch, seq_len, vocab_size)`, where the `[i, j, :]`th element is a vector of logits representing a prediction for the `j+1`th token.\n",
        "\n",
        "In this case, our batch dimension is 1, and we want to predict the token that follows after all the tokens in the sequence, hence we want to take `logits[0, -1, :]`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjbqwrc1IYRK"
      },
      "source": [
        "### Greedy Search\n",
        "\n",
        "Implement `greedy_search`, which just returns the most likely next token. If multiple tokens are equally likely, break the tie by returning the smallest token.\n",
        "\n",
        "Why not break ties randomly? It's nice that greedy search is deterministic, and also nice to not have special code for a case that rarely occurs (floats are rarely exactly equal).\n",
        "\n",
        "Tip: the type checker doesn't know the return type of `item()` is int, but you can assert that it really is an int and this will make the type checker happy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WypkwjuRIRa1",
        "outputId": "86407912-30c5-460a-8a19-74a36ea942ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy decoding with prompt:  Jingle bells, jingle bells, jingle all the way\n",
            "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
            "Greedy decoding a second time (should be deterministic): \n",
            "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def greedy_search(logits: t.Tensor) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, )\n",
        "\n",
        "    Return: the most likely token (as an integer)\n",
        "    '''\n",
        "    \n",
        "    return t.argmax(logits).item()\n",
        "\n",
        "\n",
        "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
        "print(\"Greedy decoding with prompt: \", prompt)\n",
        "output = sample_tokens(gpt2, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
        "print(f\"Your model said: {output}\")\n",
        "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
        "assert output == expected\n",
        "\n",
        "print(\"Greedy decoding a second time (should be deterministic): \")\n",
        "output = sample_tokens(gpt2, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
        "print(f\"Your model said: {output}\")\n",
        "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
        "assert output == expected\n",
        "\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pKC8XKlIa2G"
      },
      "source": [
        "## Sampling with Categorical\n",
        "\n",
        "PyTorch provides a [`distributions` package](https://pytorch.org/docs/stable/distributions.html#distribution) with a number of convenient methods for sampling from various distributions.\n",
        "\n",
        "For now, we just need [`t.distributions.categorical.Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical). Use this to implement `sample_basic`, which just samples from the provided logits (which may have already been modified by the temperature and frequency penalties).\n",
        "\n",
        "Note that this will be slow since we aren't batching the samples, but don't worry about speed for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBbEE4-uIiB4"
      },
      "source": [
        "### Basic Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HXqPMNKfIjTQ",
        "outputId": "344baac4-b6cc-4ca1-ab7e-fd065fd0b586"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.1005, 0.1946, 0.3048, 0.4001])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_basic(logits: t.Tensor) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "\n",
        "    Return: a sampled token\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    dist = t.distributions.categorical.Categorical(logits = logits)\n",
        "    return dist.sample()\n",
        "\n",
        "\n",
        "N = 20000\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_basic(unnormalized_logits) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, probs, atol=0.01, rtol=0)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIftbcLI5lW"
      },
      "source": [
        "### Temperature\n",
        "\n",
        "Temperature sounds fancy, but it's literally just dividing the logits by the temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "b82R4G7fJAgZ",
        "outputId": "7352de34-b29d-4c89-84a3-94936f212d1f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
            "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def apply_temperature(logits: t.Tensor, temperature: float) -> t.Tensor:\n",
        "    '''\n",
        "    logits: shape (vocab_size, )\n",
        "\n",
        "    Return: shape (vocab_size, )\n",
        "    '''\n",
        "    assert temperature > 0\n",
        "    logits = logits / temperature\n",
        "    return logits\n",
        "\n",
        "    \n",
        "logits = t.tensor([1, 2]).log()\n",
        "cold_logits = apply_temperature(logits, 0.001)\n",
        "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
        "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
        "hot_logits = apply_temperature(logits, 1000.0)\n",
        "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
        "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM58eNyoJOMj"
      },
      "source": [
        "### Frequency Penalty\n",
        "\n",
        "The frequency penalty is simple as well: count the number of occurrences of each token, then subtract `freq_penalty` for each occurrence. Hint: use `t.bincount` (documentation [here](https://pytorch.org/docs/stable/generated/torch.bincount.html)) to do this in a vectorized way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSWsdDpGMpme"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm getting a RuntimeError; my tensor sizes don't match.</summary>\n",
        "\n",
        "Look at the documentation page for `t.bincount`. You might need to use the `minlength` argument - why?\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DXbJlP2OMtYf",
        "outputId": "684da6ef-3d58-4bcd-d35f-a913ee2ede37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def apply_freq_penalty(input_ids: t.Tensor, logits: t.Tensor, freq_penalty: float) -> t.Tensor:\n",
        "    '''\n",
        "    input_ids: shape (seq, )\n",
        "    logits: shape (vocab_size, )\n",
        "    Return: shape (vocab_size, )\n",
        "    '''\n",
        "    \n",
        "    vocab_size = logits.shape[0]\n",
        "    freq = t.bincount(input_ids, minlength=vocab_size)\n",
        "    return logits - freq * freq_penalty\n",
        "\n",
        "    \n",
        "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
        "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\").squeeze()\n",
        "logits = t.ones(tokenizer.vocab_size)\n",
        "penalized_logits = apply_freq_penalty(input_ids, logits, 2.0)\n",
        "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space\"\n",
        "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space\"\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vKujB7Mvnm"
      },
      "source": [
        "### Sampling - Manual Testing\n",
        "\n",
        "Run the below cell to get a sense for the `temperature` and `freq_penalty` arguments. Play with your own prompt and try other values.\n",
        "\n",
        "Note: your model can generate newlines or non-printing characters, so calling `print` on generated text sometimes looks awkward on screen. You can call `repr` on the string before printing to have the string escaped nicely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "WBmw5h0iMwoY",
        "outputId": "30255c5a-874e-4fda-bdd5-83d687f35f52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0 with: High freq penalty ({'freq_penalty': 100.0}):\n",
            "Your model said: \"Jingle bells, jingle bells, jingle all the way bell. Visit Sugaree's Scarf Shop if you enjoy eating under contract loveshow!! Or click here to get\"\n",
            "\n",
            "Sample 0 with: Negative freq penalty ({'freq_penalty': -3.0}):\n",
            "Your model said: 'Jingle bells, jingle bells, jingle all the way, bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell'\n",
            "\n",
            "Sample 0 with: Too hot! ({'temperature': 2.0}):\n",
            "Your model said: 'Jingle bells, jingle bells, jingle all the way bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell bell'\n",
            "\n",
            "Sample 0 with: Pleasantly cool ({'temperature': 0.7}):\n",
            "Your model said: 'Jingle bells, jingle bells, jingle all the way.\\n\\nI was never gonna do that. I never gonna do that. I never gonna do that. I never'\n",
            "\n",
            "Sample 0 with: Pleasantly warm ({'temperature': 0.9}):\n",
            "Your model said: 'Jingle bells, jingle bells, jingle all the way. Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit Visit'\n",
            "\n",
            "Sample 0 with: Too cold! ({'temperature': 0.01}):\n",
            "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\\n\\nThe first time I saw the mountain, I was in the middle of'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "N_RUNS = 1\n",
        "your_prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
        "cases = [\n",
        "    (\"High freq penalty\", dict(freq_penalty=100.0)),\n",
        "    (\"Negative freq penalty\", dict(freq_penalty=-3.0)),\n",
        "    (\"Too hot!\", dict(temperature=2.0)),\n",
        "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
        "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
        "    (\"Too cold!\", dict(temperature=0.01)),\n",
        "]\n",
        "for (name, kwargs) in cases:\n",
        "    for i in range(N_RUNS):\n",
        "        output = sample_tokens(gpt2, tokenizer, your_prompt, max_tokens_generated=24, **kwargs)\n",
        "        print(f\"Sample {i} with: {name} ({kwargs}):\")\n",
        "        print(f\"Your model said: {repr(output)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EqkPrR1MyOc"
      },
      "source": [
        "## Top-K Sampling\n",
        "\n",
        "Conceptually, the steps in top-k sampling are:\n",
        "- Find the `top_k` largest probabilities\n",
        "- Set all other probabilities to zero\n",
        "- Normalize and sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-TRYHDMzOW"
      },
      "source": [
        "Your implementation should stay in log-space throughout (don't exponentiate to obtain probabilities). This means you don't actually need to worry about normalizing, because `Categorical` accepts unnormalised logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qg9mHz1M17l"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I don't know what function I should use for finding the top k.</summary>\n",
        "\n",
        "Use [`t.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "iFicbedJM4gu",
        "outputId": "876dd0bb-b75c-4706-c51f-52624b239826"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2180, 0.3327, 0.4494])\n",
            "Tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_top_k(logits: t.Tensor, top_k: int) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "    top_k: only consider this many of the most likely tokens for sampling\n",
        "\n",
        "    Return: a sampled token\n",
        "    '''\n",
        "    \n",
        "    topk_pred_tokens = t.topk(logits, k = top_k)\n",
        "    pred_distr = t.distributions.categorical.Categorical(logits = topk_pred_tokens.values)\n",
        "    return topk_pred_tokens.indices[pred_distr.sample().item()].item()\n",
        "\n",
        "\n",
        "k = 3\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_top_k(unnormalized_logits, k) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "expected = probs.clone()\n",
        "expected[:-k] = 0\n",
        "expected /= expected.sum()\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, expected, atol=0.01, rtol=0)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pznf6IEM6qr"
      },
      "source": [
        "### Top-K Sampling - Example\n",
        "\n",
        "The [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) famously included an example prompt about unicorns. Now it's your turn to see just how cherry picked this example was.\n",
        "\n",
        "The paper claims they used `top_k=40` and best of 10 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "uPN7GdFzM7dr",
        "outputId": "f105c3fa-5d56-4744-aa6d-63fa08d6bf8b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model said: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "The team of researchers led by the University of Oxford and the University of Utah found that the unicorns spoke English as well as Spanish in the valley where they lived. This means that they are the first unicorns to be recorded as speaking perfect English.\n",
            "\n",
            "The findings have been published in the journal PLOS\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
        "output = sample_tokens(gpt2, tokenizer, your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
        "print(f\"Your model said: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Puu82baM-IO"
      },
      "source": [
        "## Top-p aka Nucleus Sampling\n",
        "\n",
        "The basic idea is that we choose the most likely words, up until the total probability of words we've chosen crosses some threshold. Then we sample from those chosen words based on their logits.\n",
        "\n",
        "The steps are:\n",
        "\n",
        "- Sort the probabilities from largest to smallest\n",
        "- Find the cutoff point where the cumulative probability first equals or exceeds `top_p`. We do the cutoff inclusively, keeping the first probability above the threshold.\n",
        "- If the number of kept probabilities is less than `min_tokens_to_keep`, keep that many tokens instead.\n",
        "- Set all other probabilities to zero\n",
        "- Normalize and sample\n",
        "\n",
        "Optionally, refer to the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf) for some comparison of different methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYAZIYgiM_kD"
      },
      "source": [
        "#### Exercise - implement `sample_top_p`\n",
        "\n",
        "<details>\n",
        "<summary>Example of top-p sampling (if you're confused)</summary>\n",
        "\n",
        "If our probabilities were `(0.4, 0.3, 0.2, 0.1)` and our cutoff was `top_p=0.8`, then we'd sample from the first three elements (because their total probability is `0.9` which is over the threshold, but the first two only have a total prob of `0.7` which is under the threshold). Once we've chosen to sample from those three, we would renormalise them by dividing by their sum (so the probabilities we use when sampling are `(4/9, 3/9, 2/9)`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm stuck on how to implement this function.</summary>\n",
        "\n",
        "First, sort the logits using the `sort(descending=True)` method (this returns values and indices). Then you can get `cumulative_probs` by applying softmax to these logits and taking the cumsum. Then, you can decide how many probabilities to keep by using the `t.searchsorted` function.\n",
        "    \n",
        "Once you've decided which probabilities to keep, it's easiest to sample from them using the original logits (you should have preserved the indices when you called `logits.sort`). This way, you don't need to worry about renormalising like you would if you were using probabilities.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "pOImYv1oNM_-",
        "outputId": "207e9976-852a-4cff-c016-6adfd3090f47"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top_p of 0.5 or lower should only return token 2:  tensor([0., 0., 1.])\n",
            "top_p in (0.5, 0.8] should return tokens 1 and 2:  tensor([0.0000, 0.3685, 0.6315])\n",
            "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2198, 0.3304, 0.4498])\n",
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "def sample_top_p(logits: t.Tensor, top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
        "    '''\n",
        "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
        "    Return: a sampled token\n",
        "    '''\n",
        "\n",
        "    softmax_probs = t.softmax(logits.unsqueeze(0), dim = -1)\n",
        "    sorted_probs = softmax_probs.sort(descending = True)\n",
        "    cumsum_probs = t.cumsum(sorted_probs.values, dim = -1)\n",
        "    thresh_index = t.searchsorted(cumsum_probs.squeeze(0), top_p).item() + 1\n",
        "    keep_tokens = max(thresh_index, min_tokens_to_keep)\n",
        "    indices = sorted_probs.indices.squeeze(0)\n",
        "    indices_to_keep = indices[:keep_tokens]\n",
        "    #logits = sorted_probs.values.squeeze(0)\n",
        "    logits = logits[indices_to_keep]\n",
        "\n",
        "    distr = t.distributions.categorical.Categorical(logits = logits)\n",
        "    sample = indices_to_keep[distr.sample()]\n",
        "    return sample.item()\n",
        "\n",
        "    \n",
        "N = 2000\n",
        "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, 0.5) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
        "print(\"top_p of 0.5 or lower should only return token 2: \", counts)\n",
        "assert counts[0] == 0 and counts[1] == 0\n",
        "\n",
        "N = 2000\n",
        "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, 0.50001) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
        "print(\"top_p in (0.5, 0.8] should return tokens 1 and 2: \", counts)\n",
        "assert counts[0] == 0\n",
        "\n",
        "N = 5000\n",
        "top_p = 0.71\n",
        "probs = t.linspace(0, 0.4, 5)\n",
        "unnormalized_logits = probs.log() + 1.2345\n",
        "samples = t.tensor([sample_top_p(unnormalized_logits, top_p) for _ in range(N)])\n",
        "counts = t.bincount(samples, minlength=len(probs)) / N\n",
        "expected = probs.clone()\n",
        "expected[0:2] = 0\n",
        "expected /= expected.sum()\n",
        "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
        "t.testing.assert_close(counts, expected, atol=0.01, rtol=0.0)\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUsDwpHPNQDt"
      },
      "source": [
        "### Top-p Sampling - Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "El1QKmpuNQxt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d250627c-247d-43c6-cec7-f05d64e7a006"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model said: 'Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for his work on the New York Times best-seller \"The Art of Intelligence: Why Artificial Intelligence Can Make Us Better Scientists,\" as well as for his books and books on AI and social engineering. He is a graduate of the University of Pennsylvania. His latest book is \"The Future of Human Intelligence,\" and the following books'\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
        "output = sample_tokens(gpt2, tokenizer, your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64)\n",
        "print(f\"Your model said: {repr(output)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3mItbKVVm4Rx",
        "w1x_mWI6m4Rz",
        "PCZEow3yH3W8",
        "8JkIgfH5ONAA"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "arena",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "0575d7a87c0e74eddddcbc1a627da1d71db6b89a3121c036174dfb29a1bf0df3"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b805df6f8a84f4897564f152ccb62d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3cd3727f92c480885c47d80eb36ff83",
              "IPY_MODEL_612eade057c0451ab59492aa873b6be3",
              "IPY_MODEL_e1b05088b1dc4044b03f9e88311d1e21"
            ],
            "layout": "IPY_MODEL_1f53a0ddd50c4decaefacffd374854cc"
          }
        },
        "d3cd3727f92c480885c47d80eb36ff83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75e6da06147949118c6f60a62765d07f",
            "placeholder": "​",
            "style": "IPY_MODEL_f3e34100a91b432fb584575524b0ca1d",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "612eade057c0451ab59492aa873b6be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a9a3cbd7bd4c5ba70a6e225528b7b0",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1811a9693f7435c9d3ddf96f518a1d5",
            "value": 665
          }
        },
        "e1b05088b1dc4044b03f9e88311d1e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0540262086bb47b18395e9d532cc7e46",
            "placeholder": "​",
            "style": "IPY_MODEL_19e2ff04624f40da8badc19f0d51656f",
            "value": " 665/665 [00:00&lt;00:00, 43.8kB/s]"
          }
        },
        "1f53a0ddd50c4decaefacffd374854cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e6da06147949118c6f60a62765d07f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e34100a91b432fb584575524b0ca1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3a9a3cbd7bd4c5ba70a6e225528b7b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1811a9693f7435c9d3ddf96f518a1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0540262086bb47b18395e9d532cc7e46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e2ff04624f40da8badc19f0d51656f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9a3ef80dcf84c83a3c365eb3a025655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d652abc12496470a8c59e3577252c6cc",
              "IPY_MODEL_e59f46865a3843da82da5879ed577fec",
              "IPY_MODEL_988392326d424718aa544f726a0d7168"
            ],
            "layout": "IPY_MODEL_8267ce0a547f44e5bd2362d86f405e55"
          }
        },
        "d652abc12496470a8c59e3577252c6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6077381c0b984bc5b4891d3f68edce0d",
            "placeholder": "​",
            "style": "IPY_MODEL_d625cfda6f2d48dc96197104f2bcc28a",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "e59f46865a3843da82da5879ed577fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_882af7fbe28a456781d6f3efc8f967e3",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4624804f4db94a7b9bd7d1b449be872a",
            "value": 548118077
          }
        },
        "988392326d424718aa544f726a0d7168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4c379b742f4153bd5d5d85b29405df",
            "placeholder": "​",
            "style": "IPY_MODEL_090a181096464e9e876b4aa0a052f406",
            "value": " 548M/548M [00:09&lt;00:00, 81.0MB/s]"
          }
        },
        "8267ce0a547f44e5bd2362d86f405e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6077381c0b984bc5b4891d3f68edce0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d625cfda6f2d48dc96197104f2bcc28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "882af7fbe28a456781d6f3efc8f967e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4624804f4db94a7b9bd7d1b449be872a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4c379b742f4153bd5d5d85b29405df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090a181096464e9e876b4aa0a052f406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26f332178bea48ce9aee1b003b567329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_345355ba2b2d4218bd00d6e468cf2965",
              "IPY_MODEL_2f8ae10bbee94b809bdfe40b44e4ca38",
              "IPY_MODEL_efe859945acc43589bc0919383ac595f"
            ],
            "layout": "IPY_MODEL_477b22e3d2844d66910f0de6fee25064"
          }
        },
        "345355ba2b2d4218bd00d6e468cf2965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ed6d846349641608dd2a70e0bb192cc",
            "placeholder": "​",
            "style": "IPY_MODEL_0b1f665ecfdd4d3db28ae68acbaf2229",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "2f8ae10bbee94b809bdfe40b44e4ca38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c8678ed23848d992481298a9a24943",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2b88fe75a64233a54d688e9132fc21",
            "value": 124
          }
        },
        "efe859945acc43589bc0919383ac595f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d75c860a320414a84a4b4a786c4b0ee",
            "placeholder": "​",
            "style": "IPY_MODEL_690914c6b7084183b8292519feae2c47",
            "value": " 124/124 [00:00&lt;00:00, 3.95kB/s]"
          }
        },
        "477b22e3d2844d66910f0de6fee25064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed6d846349641608dd2a70e0bb192cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b1f665ecfdd4d3db28ae68acbaf2229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c8678ed23848d992481298a9a24943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2b88fe75a64233a54d688e9132fc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d75c860a320414a84a4b4a786c4b0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690914c6b7084183b8292519feae2c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1a88370757d49c79515f85167640add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df64c2cc13f748599b858f4bcd67f4e4",
              "IPY_MODEL_044d7f7cf69b40bb89c302a31bb78491",
              "IPY_MODEL_4840ef7dab5740ed8f1762451088f2a8"
            ],
            "layout": "IPY_MODEL_867f8524eb0a475f95ea21032ebbd193"
          }
        },
        "df64c2cc13f748599b858f4bcd67f4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aaf9599f2c44d35bb7ebf25c9d11ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_ec1188635d214e13a9245a7e9188050a",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "044d7f7cf69b40bb89c302a31bb78491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83252e75857149ce9d5a211966b2dc96",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9584018a4ae4ea3a8cbf9d4264da4d3",
            "value": 1042301
          }
        },
        "4840ef7dab5740ed8f1762451088f2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8bd7d210574bab94a42f341d436aed",
            "placeholder": "​",
            "style": "IPY_MODEL_1ed9becf4af547a38b6c704ad63d71f0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.24MB/s]"
          }
        },
        "867f8524eb0a475f95ea21032ebbd193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aaf9599f2c44d35bb7ebf25c9d11ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1188635d214e13a9245a7e9188050a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83252e75857149ce9d5a211966b2dc96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9584018a4ae4ea3a8cbf9d4264da4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa8bd7d210574bab94a42f341d436aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed9becf4af547a38b6c704ad63d71f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a6379653efc489aa3bdd26773cb2490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e29bcb0b09e4f7f8faa8378f8670c74",
              "IPY_MODEL_704eac3f771846479d11fef160a71bac",
              "IPY_MODEL_5ec84f20faae40118e6051f322bc1881"
            ],
            "layout": "IPY_MODEL_d9e282669341488a9ce57c3923f2bc2a"
          }
        },
        "6e29bcb0b09e4f7f8faa8378f8670c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99029029d15a4cf1afc41a2603d08294",
            "placeholder": "​",
            "style": "IPY_MODEL_42b609e1c91542d89efd98f95deb4e3d",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "704eac3f771846479d11fef160a71bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590b6a3606ac4616af104773b27b7d15",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b68df2df8f4f425a96fa1b4475e539de",
            "value": 456318
          }
        },
        "5ec84f20faae40118e6051f322bc1881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fbdda9dc7c64362b6047e239d0d2f2d",
            "placeholder": "​",
            "style": "IPY_MODEL_2433d41c34fc4d4f82cc23179508236b",
            "value": " 456k/456k [00:00&lt;00:00, 8.69MB/s]"
          }
        },
        "d9e282669341488a9ce57c3923f2bc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99029029d15a4cf1afc41a2603d08294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42b609e1c91542d89efd98f95deb4e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "590b6a3606ac4616af104773b27b7d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68df2df8f4f425a96fa1b4475e539de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fbdda9dc7c64362b6047e239d0d2f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2433d41c34fc4d4f82cc23179508236b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb68321b4ca34f3b8d48373d58c0d99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f75d943de605474b846dcdc50a190baf",
              "IPY_MODEL_df4aacaeda724537ab710685ff9a8148",
              "IPY_MODEL_a5b9066961b749bab3b337c9faf8e8ed"
            ],
            "layout": "IPY_MODEL_a309cb87c66143c7ba9d33404983d510"
          }
        },
        "f75d943de605474b846dcdc50a190baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e951b3b0d4428bb963f94a4514d20f",
            "placeholder": "​",
            "style": "IPY_MODEL_969abd4a40234309b0c422e1a8315cdd",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "df4aacaeda724537ab710685ff9a8148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330352ccf69048bdab6410304f83a9b4",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_284d7765ba0047b8bbe2a3847a0e3d4b",
            "value": 1355256
          }
        },
        "a5b9066961b749bab3b337c9faf8e8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb903806a7c04833b94c356062021750",
            "placeholder": "​",
            "style": "IPY_MODEL_c5de1a5a70ba47af82dc9f98e7334f1f",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.41MB/s]"
          }
        },
        "a309cb87c66143c7ba9d33404983d510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e951b3b0d4428bb963f94a4514d20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969abd4a40234309b0c422e1a8315cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330352ccf69048bdab6410304f83a9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "284d7765ba0047b8bbe2a3847a0e3d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb903806a7c04833b94c356062021750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5de1a5a70ba47af82dc9f98e7334f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}